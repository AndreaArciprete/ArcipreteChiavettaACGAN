{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACGAN_CIFAR10_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49657898494d4cdbae85e98638184ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5e4ff8e61f904acd98d969ea02f4e80b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb9ae43e1f214fdfa9609ffb26ffe87e",
              "IPY_MODEL_6269a2ad787c484083114f7a1632213a"
            ]
          }
        },
        "5e4ff8e61f904acd98d969ea02f4e80b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb9ae43e1f214fdfa9609ffb26ffe87e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce3113607d804722aa58af401a2a7758",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd5ca7ebddf74d21b5a0f0adf5ee493c"
          }
        },
        "6269a2ad787c484083114f7a1632213a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_54460c586ef646918ccd4411c2b7647d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:07&lt;00:00, 22132269.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b103dab66eb4685856df1f7f995d06d"
          }
        },
        "ce3113607d804722aa58af401a2a7758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd5ca7ebddf74d21b5a0f0adf5ee493c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54460c586ef646918ccd4411c2b7647d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b103dab66eb4685856df1f7f995d06d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE8v1l6Z1rVZ"
      },
      "source": [
        "#Import librerie di cui necessitiamo\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm97zOg02CDv"
      },
      "source": [
        "\"\"\"Creiamo una directory in cui andremo a salvare tutte le immagini che man mano\n",
        "verranno generate in fase di training del modello per valutare quanto il modello\n",
        "sta funzionando bene\"\"\"\n",
        "os.makedirs(\"results_images\", exist_ok=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4mEhiFx2p_P",
        "outputId": "caba3c57-93ca-438c-85b6-a9fb8f4932a2"
      },
      "source": [
        "\"\"\"Sfruttando la libreria argparse creiamo una sorta di namespace contenente\n",
        "tutte le variabili che ci serviranno da ora in avanti\"\"\"\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=70, help=\"number of epochs of training\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=4, help=\"number of cpu threads to use during batch generation\")\n",
        "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
        "parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\n",
        "parser.add_argument(\"--img_size\", type=int, default=64, help=\"size of each image dimension\")\n",
        "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=10, help=\"interval between image sampling\")\n",
        "opt = parser.parse_args()\n",
        "print(opt)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=3, f='/root/.local/share/jupyter/runtime/kernel-80047dcd-660d-4b2a-a487-fba237acb3f5.json', img_size=64, latent_dim=100, lr=0.0002, n_classes=10, n_cpu=4, n_epochs=70, sample_interval=10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z1BWBDM32bP"
      },
      "source": [
        "\"\"\"Al reference cuda assegniamo True se la GPU è disponibile, altrimenti gli\n",
        "assegniamo False\"\"\"\n",
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0aDjN4z4BtO"
      },
      "source": [
        "\"\"\"Tale funzione come parametro riceve un modulo presente all'interno del\n",
        "generatore o del discriminatore e se il modulo è di tipo Conv o sottotipo di\n",
        "Conv (ad esempio Conv2d) allora vengono inizializzati i pesi di tale modulo, se\n",
        "il modulo è di tipo BatchNorm2d allora vengono inizializzati i pesi e i bias di\n",
        "tale modulo\"\"\"\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bZeGHVzOo0f"
      },
      "source": [
        "\"\"\"Modello del generatore\"\"\"\n",
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Generator,self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(opt.n_classes, opt.latent_dim)\n",
        "        \n",
        "        #input 100*1*1\n",
        "        self.layer1 = nn.Sequential(nn.ConvTranspose2d(100,512,kernel_size=4,stride=1,padding=0,bias = False),\n",
        "                                   nn.ReLU(True))\n",
        "\n",
        "        #input 512*4*4\n",
        "        self.layer2 = nn.Sequential(nn.ConvTranspose2d(512,256,kernel_size=4,stride=2,padding=1,bias = False),\n",
        "                                   nn.BatchNorm2d(256),\n",
        "                                   nn.ReLU(True))\n",
        "        #input 256*8*8\n",
        "        self.layer3 = nn.Sequential(nn.ConvTranspose2d(256,128,kernel_size=4,stride=2,padding=1,bias = False),\n",
        "                                   nn.BatchNorm2d(128),\n",
        "                                   nn.ReLU(True))\n",
        "        #input 128*16*16\n",
        "        self.layer4 = nn.Sequential(nn.ConvTranspose2d(128,64,kernel_size=4,stride=2,padding=1,bias = False),\n",
        "                                   nn.BatchNorm2d(64),\n",
        "                                   nn.ReLU(True))\n",
        "        #input 64*32*32\n",
        "        self.layer5 = nn.Sequential(nn.ConvTranspose2d(64,3,kernel_size=4,stride=2,padding=1,bias = False),\n",
        "                                   nn.Tanh())\n",
        "        #output 3*64*64\n",
        "        \n",
        "        \n",
        "    def forward(self,noise,label):\n",
        "        \n",
        "        label_embedding = self.embedding(label)\n",
        "        x = torch.mul(noise,label_embedding)\n",
        "        x = x.view(-1,100,1,1)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkH4w1H9Ql9u"
      },
      "source": [
        "\"\"\"Modello del discriminatore\"\"\"\n",
        "class Discriminator(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Discriminator,self).__init__()        \n",
        "        \n",
        "        #input 3*64*64\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(3,64,kernel_size=4,stride=2,padding=1,bias = False),\n",
        "                                    nn.BatchNorm2d(64),\n",
        "                                   nn.LeakyReLU(0.2,True),\n",
        "                                   nn.Dropout2d(0.5))\n",
        "        \n",
        "        #input 64*32*32\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(64,128,kernel_size=4,stride=2,padding=1,bias = False),\n",
        "                                    nn.BatchNorm2d(128),\n",
        "                                   nn.LeakyReLU(0.2,True),\n",
        "                                   nn.Dropout2d(0.5))\n",
        "        #input 128*16*16\n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(128,256,kernel_size=4,stride=2,padding=1,bias = False),\n",
        "                                    nn.BatchNorm2d(256),\n",
        "                                   nn.LeakyReLU(0.2,True),\n",
        "                                   nn.Dropout2d(0.5))\n",
        "        #input 256*8*8\n",
        "        self.layer4 = nn.Sequential(nn.Conv2d(256,512,kernel_size=4,stride=2,padding=1,bias = False),\n",
        "                                    nn.BatchNorm2d(512),\n",
        "                                   nn.LeakyReLU(0.2,True))\n",
        "        #input 512*4*4\n",
        "        self.adv_layer = nn.Sequential(nn.Conv2d(512,1,kernel_size=4,stride=1,padding=0,bias = False),\n",
        "                                   nn.Sigmoid())\n",
        "        \n",
        "        self.aux_layer = nn.Sequential(nn.Conv2d(512,11,kernel_size=4,stride=1,padding=0,bias = False),\n",
        "                                   nn.LogSoftmax(dim = 1))\n",
        "        \n",
        "    def forward(self,x):\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        validity = self.adv_layer(x)\n",
        "        label = self.aux_layer(x)\n",
        "        validity = validity.view(-1,1)\n",
        "        label = label.view(-1,opt.n_classes+1)\n",
        "        \n",
        "        return validity,label"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhqqllyTW8xc"
      },
      "source": [
        "#Definiamo le Loss Function che ci serviranno\"\"\"\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "auxiliary_loss = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNHLRUb1XcOL"
      },
      "source": [
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXYN7XQGXeE7"
      },
      "source": [
        "\"\"\"Se la GPU è disponibile spostiamo i modelli e le Loss all'interno della GPU\"\"\"\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    auxiliary_loss.cuda()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiL4hM7LXqF0",
        "outputId": "9c09fb1c-890b-4304-a988-bfc6bd44e291"
      },
      "source": [
        "#Inizializziamo i parametri dei moduli presenti all'interno del generatore e del discriminatore\n",
        "\"\"\"Supponiamo di considerare la funzione apply invocata su generator. Tale funzione\n",
        "in maniera iterativa prende ogni singolo modulo presente all'interno di generator\n",
        "e lo passa come parametro della funzione weights_init_normal\"\"\"\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (layer1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (3): Dropout2d(p=0.5, inplace=False)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (3): Dropout2d(p=0.5, inplace=False)\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (3): Dropout2d(p=0.5, inplace=False)\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "  )\n",
              "  (adv_layer): Sequential(\n",
              "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (aux_layer): Sequential(\n",
              "    (0): Conv2d(512, 11, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (1): LogSoftmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa2IObdQYLVL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "49657898494d4cdbae85e98638184ee7",
            "5e4ff8e61f904acd98d969ea02f4e80b",
            "bb9ae43e1f214fdfa9609ffb26ffe87e",
            "6269a2ad787c484083114f7a1632213a",
            "ce3113607d804722aa58af401a2a7758",
            "cd5ca7ebddf74d21b5a0f0adf5ee493c",
            "54460c586ef646918ccd4411c2b7647d",
            "4b103dab66eb4685856df1f7f995d06d"
          ]
        },
        "outputId": "ce9f8344-8d3d-4afd-feb7-be15420981de"
      },
      "source": [
        "# Configure data loader\n",
        "os.makedirs(\"../../data/CIFAR10\", exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(\n",
        "        \"../../data/CIFAR10\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/CIFAR10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49657898494d4cdbae85e98638184ee7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../../data/CIFAR10/cifar-10-python.tar.gz to ../../data/CIFAR10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWOVuAJsYhrz"
      },
      "source": [
        "#Creiamo gli ottimizzatori\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW_Xf7BfYr09"
      },
      "source": [
        "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iALlIdR3Y6mM"
      },
      "source": [
        "def sample_image(n_row, epoch):\n",
        "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
        "    # Sample noise\n",
        "    \"\"\"Per generare z, ossia il minibatch contenente rumore casuale da mandare in ingresso\n",
        "    al generatore abbiamo creato un Tensor 2-D avente un numero di righe pari a\n",
        "    n_row**2 e un numero di colonne pari a 100. Dopodiché, dal momento che vogliamo\n",
        "    che questo Tensor contenga sempre gli stessi valori anche se la funzione sample_image\n",
        "    viene chiamata più volte abbiamo deciso di wrappare tale Tensor all'interno di Variable.\n",
        "    Così facendo, anche se la funzione sample_image viene chiamata più volte, il valore di z\n",
        "    rimane invariato\"\"\"\n",
        "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
        "    # Get labels ranging from 0 to n_classes for n rows\n",
        "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
        "    labels = Variable(LongTensor(labels))\n",
        "    \"\"\"Per generare labels, ossia il minibatch contenente i target da mandare in ingresso\n",
        "    al generatore abbiamo creato un Tensor 1-D avente un numero di elementi pari a 100\n",
        "    (numero di righe di z), dove l'elemento di indice 0 assume 0, l'elemento di indice 1 assume 1, l'elemento di indice 2 assume 2\n",
        "    l'elemento di indice 9 assume 9, l'elemento di indice 10 assume 0, l'elemento di indice 11 assume 1 e così via.\n",
        "    Inoltre, per far si che il valore di labels non cambi nonostante la funzione sample_image venga chiamata più volte abbiamo wrappato\n",
        "    tale Tensor all'interno di Variable\"\"\"\n",
        "    \"\"\"Prendiamo il Tensor 2-D z e il Tensor 1-D labels e mandiamoli in ingresso al generatore\"\"\"\n",
        "    gen_imgs = generator(z, labels)\n",
        "    \"\"\"Il numero di immmagini che vengono prodotte in uscita dal gneratore è pari a n_row**2\"\"\"\n",
        "    save_image(gen_imgs.data, \"results_images/Epoca%d.png\" % epoch, nrow=n_row, normalize=True)\n",
        "    \"\"\"Tramite save_image all'interno della directory results_images che abbiamo creato\n",
        "    precedentemente inseriamo una griglia costituita dalle n_row**2 immagini prodotte in uscita\n",
        "    dal generatore disposte su n_row righe\"\"\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkbdEPGKnM5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0119a03-05fe-4b8a-9cd3-108dbdc060c5"
      },
      "source": [
        "for epoch in range(opt.n_epochs):\n",
        "  history_d_minibatch_accuracy = []\n",
        "  history_d_minibatch_loss = []\n",
        "  history_g_minibatch_loss = []\n",
        "  for i, (imgs, labels) in enumerate(dataloader):\n",
        "\n",
        "        batch_size = imgs.shape[0] #64\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
        "        \"\"\"valid è un Tensor 2-D avente un numero di righe pari a batch_size(64)\n",
        "        e un numero di colonne pari a 1. Tale Tensor contiene tutti 1 e ci \n",
        "        serve per calcolare l'adversarial loss per le immagini reali\n",
        "        \"\"\"\n",
        "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
        "        \"\"\"fake è un Tensor 2-D avente un numero di righe pari a batch_size(64)\n",
        "        e un numero di colonne pari a 1. Tale Tensor contiene tutti 0 e ci \n",
        "        serve per calcolare l'adversarial loss per le immagini fake\n",
        "        \"\"\"\n",
        "\n",
        "        # Configure input\n",
        "        \"\"\"Invocando il metodo type su un Tensor è possibile effettuare il\n",
        "        casting di tale Tensor\"\"\"\n",
        "        real_imgs = Variable(imgs.type(FloatTensor))\n",
        "        \"\"\"real_imgs è un minibatch contenente un numero di immagini reali pari\n",
        "        a minibatch_size(64)\"\"\"\n",
        "        labels = Variable(labels.type(LongTensor))\n",
        "        \"\"\"labels è un minibatch contenente i minibatch_size(64) target\n",
        "        corrispondenti alle 64 immagini reali presenti nel minibatch real_imgs\"\"\"\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        \"\"\"Resettiamo il gradiente\"\"\"\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise and labels as generator input\n",
        "        \"\"\"z è un minibatch contenente 64 rumori casuali, ognuno di 100 elementi\"\"\"\n",
        "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
        "        \"\"\"gen_labels è un Tensor 1-D contenente 64 (batch_size) target, uno per\n",
        "        ogni singolo rumore casuale presente nel minibatch z\"\"\"\n",
        "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z, gen_labels)\n",
        "        \"\"\"gen_imgs è un minibatch contenente le 64 immagini (batch_size)\n",
        "        generate dal generatore\"\"\"\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        \"\"\"A questo punto prendiamo il minibatch contenente le 64 immagini\n",
        "        generate dal generatore e le mandiamo in ingresso al discriminatore\"\"\"\n",
        "        validity, pred_label = discriminator(gen_imgs)\n",
        "        g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
        "        \"\"\"Calcoliamo il gradiente della g_loss\"\"\"\n",
        "        g_loss.backward()\n",
        "        \"\"\"Ottimizziamo i parametri del generatore\"\"\"\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        \"\"\"Resettiamo il gradiente\"\"\"\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Loss for real images\n",
        "        real_pred, real_aux = discriminator(real_imgs)\n",
        "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
        "\n",
        "        # Loss for fake images\n",
        "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
        "        \"\"\"gen_imgs contiene le 64 (batch_size) immagini generate dal generatore\n",
        "        durante la fase di training del generatore\"\"\"\n",
        "\n",
        "        \"\"\"Utilizzando detach() nel momento in cui attuiamo la backpropagation\n",
        "        per aggiornare i parametri del discriminatore quello che accade è che\n",
        "        effettivamente vengono aggiornati solo ed esclusivamente i parametri del\n",
        "        discriminatore e non anche quelli del generatore.\n",
        "        Se non usassimo detach() nel momento in cui attuaiamo la backpropagation\n",
        "        per aggiornare i parametri del discriminatore, oltre ai parametri del\n",
        "        discriminatore, verrebbero aggiornati anche quelli del generatore e questo\n",
        "        non va bene in quanto vogliamo aggiornare solo i parametri del\n",
        "        discriminatore\"\"\"\n",
        "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "        \"\"\"Calcoliamo il gradiente della loss del discriminatore\"\"\"\n",
        "        d_loss.backward()\n",
        "        \"\"\"Ottimizziamo i parametri del discriminatore\"\"\"\n",
        "        optimizer_D.step()\n",
        "\n",
        "\n",
        "        #Calcoliamo l'accuratezza del discriminatore nel classificare correttamente la classe di appartenenza di ogni singola immagine (fake o reale) che riceve\n",
        "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
        "        \"\"\"pred ha dimensione 128x10 in quanto è dato dalla concatenazione per riga\n",
        "        di real_aux (Tensor 2-D di dimensione 64x10 contenente per ogni immagine reale fornita al discriminatore\n",
        "        le probabilità con cui l'immagine appartiene ad ognuna delle 10 classi) e di\n",
        "        fake_aux (Tensor 2-D di dimensione 64x10 contenente per ogni immagine fake fornita al discriminatore\n",
        "        le probabilità con cui l'immagine appartiene ad ognuna delle 10 classi)\"\"\"\n",
        "        target = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
        "        \"\"\"target ha dimensione 128 in quanto è dato dalla concatenazione per riga\n",
        "        di labels (Tensor 1-D di dimensione 64 contenente per ogni immagine reale fornita al discriminatore il relativo target)\n",
        "        e di gen_labels (Tensor 1-D di dimensione 64 contenente per ogni immagine fake fornita al discriminatore il relativo target)\n",
        "        \"\"\"\n",
        "        d_minibatch_accuracy = np.mean(np.argmax(pred, axis=1) == target)\n",
        "        history_d_minibatch_accuracy.append(d_minibatch_accuracy)\n",
        "        history_d_minibatch_loss.append(d_loss.item())\n",
        "        history_g_minibatch_loss.append(g_loss.item())\n",
        "\n",
        "        if (epoch+1) % opt.sample_interval == 0:\n",
        "            sample_image(n_row=10, epoch=epoch+1)\n",
        "\n",
        "  d_epoch_accuracy = np.mean(history_d_minibatch_accuracy)\n",
        "  d_epoch_loss = np.mean(history_d_minibatch_loss)\n",
        "  g_epoch_loss = np.mean(history_g_minibatch_loss)\n",
        "  print(f\"Epoch {epoch+1}: Discriminator_Loss={d_epoch_loss:.4f}, Discriminator_Accuracy={d_epoch_accuracy*100:.4f}, Generator_Loss={g_epoch_loss:.4f}\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Discriminator_Loss=1.1680, Discriminator_Accuracy=34.3151, Generator_Loss=1.8639\n",
            "Epoch 2: Discriminator_Loss=0.9019, Discriminator_Accuracy=62.9546, Generator_Loss=0.7892\n",
            "Epoch 3: Discriminator_Loss=0.8319, Discriminator_Accuracy=67.3014, Generator_Loss=0.6660\n",
            "Epoch 4: Discriminator_Loss=0.7901, Discriminator_Accuracy=70.0867, Generator_Loss=0.6093\n",
            "Epoch 5: Discriminator_Loss=0.7572, Discriminator_Accuracy=72.2077, Generator_Loss=0.5597\n",
            "Epoch 6: Discriminator_Loss=0.7306, Discriminator_Accuracy=74.3586, Generator_Loss=0.5206\n",
            "Epoch 7: Discriminator_Loss=0.7136, Discriminator_Accuracy=75.9571, Generator_Loss=0.4753\n",
            "Epoch 8: Discriminator_Loss=0.6915, Discriminator_Accuracy=77.3717, Generator_Loss=0.4616\n",
            "Epoch 9: Discriminator_Loss=0.6722, Discriminator_Accuracy=78.3628, Generator_Loss=0.4480\n",
            "Epoch 10: Discriminator_Loss=0.6529, Discriminator_Accuracy=79.4577, Generator_Loss=0.4440\n",
            "Epoch 11: Discriminator_Loss=0.6365, Discriminator_Accuracy=80.0681, Generator_Loss=0.4461\n",
            "Epoch 12: Discriminator_Loss=0.6185, Discriminator_Accuracy=80.9303, Generator_Loss=0.4490\n",
            "Epoch 13: Discriminator_Loss=0.6023, Discriminator_Accuracy=81.4108, Generator_Loss=0.4743\n",
            "Epoch 14: Discriminator_Loss=0.5689, Discriminator_Accuracy=81.9144, Generator_Loss=0.5185\n",
            "Epoch 15: Discriminator_Loss=0.5522, Discriminator_Accuracy=82.2251, Generator_Loss=0.5643\n",
            "Epoch 16: Discriminator_Loss=0.5348, Discriminator_Accuracy=82.5168, Generator_Loss=0.5987\n",
            "Epoch 17: Discriminator_Loss=0.5294, Discriminator_Accuracy=83.0113, Generator_Loss=0.6068\n",
            "Epoch 18: Discriminator_Loss=0.5245, Discriminator_Accuracy=83.2930, Generator_Loss=0.6123\n",
            "Epoch 19: Discriminator_Loss=0.5187, Discriminator_Accuracy=83.4099, Generator_Loss=0.6223\n",
            "Epoch 20: Discriminator_Loss=0.5078, Discriminator_Accuracy=83.9264, Generator_Loss=0.6299\n",
            "Epoch 21: Discriminator_Loss=0.4978, Discriminator_Accuracy=84.1292, Generator_Loss=0.6459\n",
            "Epoch 22: Discriminator_Loss=0.4990, Discriminator_Accuracy=84.4319, Generator_Loss=0.6510\n",
            "Epoch 23: Discriminator_Loss=0.4916, Discriminator_Accuracy=84.7636, Generator_Loss=0.6408\n",
            "Epoch 24: Discriminator_Loss=0.4954, Discriminator_Accuracy=84.7566, Generator_Loss=0.6477\n",
            "Epoch 25: Discriminator_Loss=0.4874, Discriminator_Accuracy=85.1343, Generator_Loss=0.6429\n",
            "Epoch 26: Discriminator_Loss=0.4915, Discriminator_Accuracy=85.3291, Generator_Loss=0.6352\n",
            "Epoch 27: Discriminator_Loss=0.4849, Discriminator_Accuracy=85.6138, Generator_Loss=0.6256\n",
            "Epoch 28: Discriminator_Loss=0.4875, Discriminator_Accuracy=85.6638, Generator_Loss=0.6303\n",
            "Epoch 29: Discriminator_Loss=0.4834, Discriminator_Accuracy=85.9865, Generator_Loss=0.6350\n",
            "Epoch 30: Discriminator_Loss=0.4801, Discriminator_Accuracy=86.3081, Generator_Loss=0.6208\n",
            "Epoch 31: Discriminator_Loss=0.4736, Discriminator_Accuracy=86.3521, Generator_Loss=0.6371\n",
            "Epoch 32: Discriminator_Loss=0.4728, Discriminator_Accuracy=86.5509, Generator_Loss=0.6328\n",
            "Epoch 33: Discriminator_Loss=0.4721, Discriminator_Accuracy=86.6468, Generator_Loss=0.6399\n",
            "Epoch 34: Discriminator_Loss=0.4656, Discriminator_Accuracy=86.9086, Generator_Loss=0.6304\n",
            "Epoch 35: Discriminator_Loss=0.4634, Discriminator_Accuracy=87.1523, Generator_Loss=0.6400\n",
            "Epoch 36: Discriminator_Loss=0.4606, Discriminator_Accuracy=87.3212, Generator_Loss=0.6395\n",
            "Epoch 37: Discriminator_Loss=0.4648, Discriminator_Accuracy=87.4930, Generator_Loss=0.6239\n",
            "Epoch 38: Discriminator_Loss=0.4606, Discriminator_Accuracy=87.4700, Generator_Loss=0.6252\n",
            "Epoch 39: Discriminator_Loss=0.4502, Discriminator_Accuracy=87.7538, Generator_Loss=0.6341\n",
            "Epoch 40: Discriminator_Loss=0.4471, Discriminator_Accuracy=87.7917, Generator_Loss=0.6476\n",
            "Epoch 41: Discriminator_Loss=0.4460, Discriminator_Accuracy=87.9835, Generator_Loss=0.6510\n",
            "Epoch 42: Discriminator_Loss=0.4479, Discriminator_Accuracy=88.2303, Generator_Loss=0.6399\n",
            "Epoch 43: Discriminator_Loss=0.4415, Discriminator_Accuracy=88.1903, Generator_Loss=0.6474\n",
            "Epoch 44: Discriminator_Loss=0.4416, Discriminator_Accuracy=88.3262, Generator_Loss=0.6611\n",
            "Epoch 45: Discriminator_Loss=0.4382, Discriminator_Accuracy=88.3552, Generator_Loss=0.6472\n",
            "Epoch 46: Discriminator_Loss=0.4379, Discriminator_Accuracy=88.7038, Generator_Loss=0.6653\n",
            "Epoch 47: Discriminator_Loss=0.4268, Discriminator_Accuracy=88.7578, Generator_Loss=0.6470\n",
            "Epoch 48: Discriminator_Loss=0.4338, Discriminator_Accuracy=88.7708, Generator_Loss=0.6583\n",
            "Epoch 49: Discriminator_Loss=0.4248, Discriminator_Accuracy=88.8887, Generator_Loss=0.6765\n",
            "Epoch 50: Discriminator_Loss=0.4259, Discriminator_Accuracy=88.9366, Generator_Loss=0.6822\n",
            "Epoch 51: Discriminator_Loss=0.4265, Discriminator_Accuracy=89.1764, Generator_Loss=0.6633\n",
            "Epoch 52: Discriminator_Loss=0.4129, Discriminator_Accuracy=89.3922, Generator_Loss=0.6748\n",
            "Epoch 53: Discriminator_Loss=0.4173, Discriminator_Accuracy=89.4102, Generator_Loss=0.6819\n",
            "Epoch 54: Discriminator_Loss=0.4178, Discriminator_Accuracy=89.3732, Generator_Loss=0.6833\n",
            "Epoch 55: Discriminator_Loss=0.4153, Discriminator_Accuracy=89.5161, Generator_Loss=0.6864\n",
            "Epoch 56: Discriminator_Loss=0.4053, Discriminator_Accuracy=89.5390, Generator_Loss=0.6873\n",
            "Epoch 57: Discriminator_Loss=0.4014, Discriminator_Accuracy=89.7428, Generator_Loss=0.6883\n",
            "Epoch 58: Discriminator_Loss=0.4075, Discriminator_Accuracy=89.8388, Generator_Loss=0.7035\n",
            "Epoch 59: Discriminator_Loss=0.3986, Discriminator_Accuracy=89.9107, Generator_Loss=0.6981\n",
            "Epoch 60: Discriminator_Loss=0.3985, Discriminator_Accuracy=89.8777, Generator_Loss=0.7240\n",
            "Epoch 61: Discriminator_Loss=0.3983, Discriminator_Accuracy=89.9057, Generator_Loss=0.7035\n",
            "Epoch 62: Discriminator_Loss=0.3988, Discriminator_Accuracy=90.0116, Generator_Loss=0.7186\n",
            "Epoch 63: Discriminator_Loss=0.3920, Discriminator_Accuracy=90.1894, Generator_Loss=0.7046\n",
            "Epoch 64: Discriminator_Loss=0.3882, Discriminator_Accuracy=90.1834, Generator_Loss=0.7282\n",
            "Epoch 65: Discriminator_Loss=0.4011, Discriminator_Accuracy=90.2783, Generator_Loss=0.7052\n",
            "Epoch 66: Discriminator_Loss=0.3900, Discriminator_Accuracy=90.3752, Generator_Loss=0.7185\n",
            "Epoch 67: Discriminator_Loss=0.3835, Discriminator_Accuracy=90.4412, Generator_Loss=0.7311\n",
            "Epoch 68: Discriminator_Loss=0.3869, Discriminator_Accuracy=90.4672, Generator_Loss=0.7265\n",
            "Epoch 69: Discriminator_Loss=0.3862, Discriminator_Accuracy=90.5810, Generator_Loss=0.7275\n",
            "Epoch 70: Discriminator_Loss=0.3813, Discriminator_Accuracy=90.7609, Generator_Loss=0.7344\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}